# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y3fNyItpCSmHIZIP9xaN9HXehTxJU7T3
"""

# !pip install tensorflow numpy pandas matplotlib seaborn xgboost scikit-learn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import gym
import networkx as nx
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Attention
from xgboost import XGBRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from tensorflow.keras.layers import Layer
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional, Input
from tensorflow.keras.callbacks import EarlyStopping

df = pd.read_csv('updated_solar.csv')
print(df.columns)

# Load dataset
df = pd.read_csv('updated_solar.csv')

# Define features & target variable
features = df[['hour_sin', 'hour_cos',
               'dayofyear_sin', 'dayofyear_cos',
               'DE_ALLSKY_SFC_SW_DNI', 'DE_T2M',
               'DE_RH2M', 'DE_PRECTOTCORR', 'DE_PS']]
target = df['DE_solar_generation_actual']

# Normalize features
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)

# Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)

# Reshape for LSTM (samples, timesteps, features)
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))

print("‚úÖ Data loaded & preprocessed!")

class Attention(Layer):
    def __init__(self):
        super(Attention, self).__init__()

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], 1),
                                 initializer="random_normal",
                                 regularizer=l2(0.01), trainable=True)
        self.b = self.add_weight(shape=(1,), initializer="zeros", trainable=True)

    def call(self, inputs):
        attention_scores = tf.nn.softmax(tf.matmul(inputs, self.W) + self.b, axis=1)
        return tf.reduce_sum(inputs * attention_scores, axis=1)

print("‚úÖ Attention Layer Ready!")

# Define BiLSTM model with Attention
def build_bilstm_model(input_shape):
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True, activation='relu', kernel_regularizer=l2(0.01)), input_shape=input_shape),
        Attention(),
        BatchNormalization(),  # Added Batch Normalization
        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),  # Added Regularization
        Dropout(0.4),  # Increased Dropout to 0.3
        Dense(1)  # Output layer
    ])

    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Build and Train the Model
model = build_bilstm_model((1, X_train.shape[2]))
model.summary()

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the Model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

print("‚úÖ BiLSTM with Attention trained successfully!")

import xgboost as xgb

# Train XGBoost
xgb_model = xgb.XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=4,
                             subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, gamma=0.3,min_child_weight=5)  # Added Regularization
xgb_model.fit(X_train.reshape(X_train.shape[0], X_train.shape[2]), y_train)
print("‚úÖ XGBoost model trained!")

# Get predictions
bilstm_preds = model.predict(X_test).flatten()
xgb_preds = xgb_model.predict(X_test.reshape(X_test.shape[0], X_test.shape[2]))

# ‚úÖ Hybrid Prediction (Weighted Averaging to Reduce Variance)
hybrid_preds = (0.6 * bilstm_preds + 0.4* xgb_preds)  # Give more weight to LSTM

# Calculate Errors
mae = mean_absolute_error(y_test, hybrid_preds)
mse = mean_squared_error(y_test, hybrid_preds)

print(f"‚úÖ Hybrid Model MAE: {mae:.2f}")
print(f"‚úÖ Hybrid Model MSE: {mse:.2f}")

class EnergyEnv(gym.Env):
    def __init__(self):
        super(EnergyEnv, self).__init__()
        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,))
        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(X_train.shape[2],))
        self.current_step = 0

    def reset(self):
        self.current_step = 0
        return X_train[self.current_step].flatten()

    def step(self, action):
        pred = hybrid_preds[self.current_step] + action[0]
        error = abs(y_test.iloc[self.current_step] - pred)

        # ‚úÖ Normalized Reward with Clipping
        reward = max(-error, -10)  # Clip rewards to avoid instability

        self.current_step += 1
        done = self.current_step >= len(y_test) - 1
        next_state = X_train[self.current_step].flatten() if not done else np.zeros(X_train.shape[2])

        return next_state, reward, done, {}

env = EnergyEnv()
state = env.reset()
print("‚úÖ Reinforcement Learning Environment Ready!")

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Compute errors
bilstm_mae = mean_absolute_error(y_test, bilstm_preds)
bilstm_rmse = np.sqrt(mean_squared_error(y_test, bilstm_preds))
bilstm_r2 = r2_score(y_test, bilstm_preds)

xgb_mae = mean_absolute_error(y_test, xgb_preds)
xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_preds))
xgb_r2 = r2_score(y_test, xgb_preds)

hybrid_mae = mean_absolute_error(y_test, hybrid_preds)
hybrid_rmse = np.sqrt(mean_squared_error(y_test, hybrid_preds))
hybrid_r2 = r2_score(y_test, hybrid_preds)

# Print results
print(" **Model Evaluation Metrics**")
print(f" BiLSTM + Attention -> MAE: {bilstm_mae:.2f}, RMSE: {bilstm_rmse:.2f}, R¬≤: {bilstm_r2:.2f}")
print(f" XGBoost -> MAE: {xgb_mae:.2f}, RMSE: {xgb_rmse:.2f}, R¬≤: {xgb_r2:.2f}")
print(f" Hybrid Model -> MAE: {hybrid_mae:.2f}, RMSE: {hybrid_rmse:.2f}, R¬≤: {hybrid_r2:.2f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

# Plot Actual vs Hybrid Predictions
plt.plot(y_test.values[:100], label="Actual", marker='o', linestyle='dashed', color='blue')
plt.plot(hybrid_preds[:100], label="Hybrid Predictions", marker='x', linestyle='dashed', color='red')

plt.title("üîç Hybrid Model Predictions vs. Actual Values")
plt.xlabel("Time Step")
plt.ylabel("Solar Generation Output")
plt.legend()
plt.show()

import seaborn as sns

residuals = y_test - hybrid_preds  # Difference between actual and predicted

plt.figure(figsize=(10, 5))
sns.histplot(residuals, kde=True, bins=30, color="purple")

plt.title(" Residual (Error) Distribution")
plt.xlabel("Prediction Error")
plt.ylabel("Frequency")
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label="Training Loss", color='blue')
plt.plot(history.history['val_loss'], label="Validation Loss", color='red')

plt.title(" Model Learning Curve: Loss vs. Epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()